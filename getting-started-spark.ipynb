{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "    \n",
    "# <font color=\"red\">Getting Started with Spark Conda Env</font>\n",
    "<p style=\"margin-left:10%; margin-right:10%;\">by the <font color=\"teal\">Workshop for Sisal</font></p>\n",
    "\n",
    "---\n",
    "---\n",
    "\n",
    "## Overview\n",
    "\n",
    "The PySpark and Data Flow conda allows you to leverage the power of Apache Spark. Use it to access the full computational power of a notebook session by using parallel computing. For larger jobs, you can interactively develop Apache Spark applications and submit them to Oracle Data Flow without blocking the notebook session. PySpark MLlib implements a wide collection of powerful machine-learning algorithms. Use the SQL-like language of PySparkSQL to analyze huge amounts of structure and semi-structured data stored on Oracle Object Storage. Speed up your workflow by using sparksql-magic to run PySparkSQL queries directly in the notebook.\n",
    "\n",
    "This notebook shows you how to authenticate OCI resources, and how to configure the `core-site.xml` file so that PySpark can access Object Storage.\n",
    "\n",
    "---\n",
    "\n",
    "## Contents:\n",
    "\n",
    "- <a href='#authentication'>Understanding Authentication to Oracle Cloud Infrastructure Resources from a Notebook Session</a>\n",
    " - <a href='#resource_principals'>Authentication with Resource Principals</a>\n",
    "    - <a href='#resource_principals_ads'>Resource Principals Authentication using the ADS SDK</a>\n",
    "    - <a href='#resource_principals_oci'>Resource Principals Authentication using the OCI SDK</a>\n",
    "    - <a href='#resource_principals_cli'>Resource Principals Authentication using the OCI CLI</a> \n",
    "- <a href='#conda'>Conda</a>\n",
    "    - <a href='#conda_overview'>Overview</a>\n",
    "    - <a href='#conda_libraries'>Principal Conda Libraries</a>\n",
    "    - <a href='#conda_configuration'>Configuration</a>\n",
    "        - <a href='#coresite_auth_rp'>Authentication with Resource Principals</a>\n",
    "           - <a href='#odsc_coresite_command_rp'>Configuration of `core-site.mxl` Using the `odsc` Command Line Tool</a>\n",
    "           - <a href='#manually_update_coresite_rp'>Manually Configurating `core-site.xml`</a>\n",
    "        - <a href='#coresite_auth_api_keys'>Authentication with API Keys</a>\n",
    "           - <a href='#odsc_coresite_command_api_keys'>Configuration of `core-site.mxl` Using the `odsc` Command Line Tool</a>\n",
    "           - <a href='#manually_update_coresite_api_keys'>Manually Configurating `core-site.xml`</a>\n",
    "        - <a href='#conda_configuration_testing'>Testing the Configuration</a>\n",
    "- <a href='#ref'>References</a> \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import warnings\n",
    "import os\n",
    "import ads\n",
    "from oci.auth.signers import get_resource_principals_signer\n",
    "from oci.data_science import DataScienceClient\n",
    "from os import path\n",
    "from os import cpu_count\n",
    "from pyspark.sql import SparkSession\n",
    "import re\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(format='%(levelname)s:%(message)s', level=logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='authentication'></a>\n",
    "# Understanding Authentication to OCI Resources from a Notebook Session\n",
    "\n",
    "When working within a notebook session, the `datascience` user is used. This user does not have an OCI Identity and Access Management (IAM) identity, so it has no access to the OCI API. To access OCI service resources, including Data Science projects and models, from a notebook environment, you must configure either resource principals or API keys. \n",
    "\n",
    "PySpark can authenticate with Object Storage using resource principals or API keys.  API keys *cannot* contain a passphrase, see [setting up keys and configuration files](https://docs.cloud.oracle.com/en-us/iaas/Content/API/Concepts/devguidesetupprereq.htm), and the `api_keys.ipynb` example notebook.\n",
    "\n",
    "If you must have a passphrase in your configuration and key files, you can download the file from Object Storage locally with the OCI Python SDK, and then load the local file in a Spark context.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='resource_principals'></a>\n",
    "## Authentication with Resource Principals \n",
    "\n",
    "Data Science enables easy and secure authentication using the notebook session's resource principal to access other OCI resources, including Data Science projects and models. The following cells show you how to use your notebook session's resource principal.\n",
    "\n",
    "In advance, a tenancy administrator must write policies to grant permissions to the resource principal to access other OCI resources, see [manually configuring your tenancy for Data Science](https://docs.cloud.oracle.com/en-us/iaas/data-science/using/configure-tenancy.htm) for details.\n",
    "\n",
    "There are two methods to configure the notebook to use resource principals, use the `ads` library or the `oci` library. While both these libraries provide the required authentication, the `ads` library is specifically designed for easy operation within a Data Science notebook session.\n",
    "\n",
    "If you don't want to take on these library dependencies, you can use the `oci` command from the command line.\n",
    "\n",
    "For more details about using resource principals in the Data Science service, see the [ADS Configuration Guide](https://docs.cloud.oracle.com/en-us/iaas/tools/ads-sdk/latest/user_guide/configuration/configuration.html#), and [authenticating to the OCI APIs from a notebook session](https://docs.cloud.oracle.com/en-us/iaas/data-science/using/use-notebook-sessions.htm#topic_kxj_znw_pkb).\n",
    "\n",
    "<a id='resource_principals_ads'></a>\n",
    "### Resource Principals Authentication using the ADS SDK\n",
    "\n",
    "The `set_auth()` method sets the proper authentication mechanism for ADS. ADS uses the `oci` SDK to access resources like the model catalog or Object Storage.\n",
    "\n",
    "Within a notebook session, you configure the use of a resource principal for the ADS SDK by running this in a notebook cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ads.set_auth(auth='resource_principal') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conda'></a>\n",
    "# Conda\n",
    "\n",
    "<a id='conda_overview'></a>\n",
    "## Overview\n",
    "\n",
    "This conda allows data scientists to leverage Apache Spark.  You can set up Apache Spark applications, and then submit them to Data Flow. You can also use PySpark, including PySpark MLib and PySparkSQL.  \n",
    "\n",
    "<a id='conda_libraries'></a>\n",
    "## Principal Conda Libraries\n",
    "\n",
    "These are some of the libraries included in this conda:\n",
    "\n",
    "- ads: Partial ADS distribution. This distribution excludes Oracle AutoML and MLX. \n",
    "- oraclejdk: Oracle Java Development Kits.\n",
    "- pyspark: Python API for Apache Spark.\n",
    "- scikit-learn: A library for building machine learning models including regressions, classifiers, and clustering algorithms.\n",
    "- sparksql-magic: A Library for SparkSQL Magic commands for Jupyter notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conda_configuration'></a>\n",
    "## Configuration\n",
    "\n",
    "To access Object Storage, the `core-site.xml` file must be configured.  \n",
    "\n",
    "`core-site.xml` can be manually configured or configured with the use of the `odsc` program.\n",
    "\n",
    "<a id='coresite_auth_rp'></a>\n",
    "### Authentication with Resource Principals\n",
    "\n",
    "<a id='odsc_coresite_command_rp'></a>\n",
    "#### Configuration of `core-site.mxl` Using the `odsc` Command Line Tool\n",
    "\n",
    "When authenticated with resource principals, you can run `odsc core-site config -o -a resource_principal`. It automatically populates `core-site.xml`, and saves the file to `~/spark_conf_dir/core-site.xml`. \n",
    "\n",
    "You can use these command line options \n",
    "- `-a`, `--authentication` Authentication mode. Supports `resource_principal` and `api_key` (default).\n",
    "- `-r`, `--region` Name of the region.\n",
    "- `-o`, `--overwrite` Overwrite `core-site.xml`.\n",
    "- `-O`, `--output` Output path for `core-site.xml`.\n",
    "- `-q`, `--quiet` Suppress non-error output.\n",
    "\n",
    "Run `odsc core-site config --help` to check the use of this CLI using the command line.\n",
    "\n",
    "<a id='manually_update_coresite_rp'></a>\n",
    "#### Manually Configuring `core-site.xml`\n",
    "When the conda package is installed, a templated version of `core-site.xml` is also installed. \n",
    "\n",
    "This file has to be updated to include the following values:\n",
    "\n",
    "`fs.oci.client.hostname`: The address of Object Storage. For example, `https://objectstorage.us-ashburn-1.oraclecloud.com` You have to replace `us-ashburn-1` with the region you are in.\n",
    "\n",
    "`fs.oci.client.custom.authenticator`: Set the value to `com.oracle.bmc.hdfs.auth.ResourcePrincipalsCustomAuthenticator`. \n",
    "\n",
    "When using resource principals, these properties don't need to be configured:\n",
    "\n",
    "- `fs.oci.client.auth.tenantId`\n",
    "- `fs.oci.client.auth.userId`\n",
    "- `fs.oci.client.auth.fingerprint`\n",
    "- `fs.oci.client.auth.pemfilepath`\n",
    "\n",
    "The following example `core-site.xml` file illustrates using resource principals for authentication to access Object Storage:\n",
    "\n",
    "```{xml}\n",
    "<?xml version=\"1.0\"?>\n",
    "<configuration>\n",
    "  <property>\n",
    "    <name>fs.oci.client.hostname</name>\n",
    "    <value>https://objectstorage.us-ashburn-1.oraclecloud.com</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>fs.oci.client.custom.authenticator</name>\n",
    "    <value>com.oracle.bmc.hdfs.auth.ResourcePrincipalsCustomAuthenticator</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "For details, see [HDFS connector for Object Storage #using resource principals for authentication](https://docs.oracle.com/en-us/iaas/Content/API/SDKDocs/hdfsconnector.htm#hdfs_using_resource_principals_for_authentication).\n",
    "\n",
    "<a id='coresite_auth_api_keys'></a>\n",
    "### Authentication with API Keys\n",
    "<a id='odsc_coresite_command_api_keys'></a>\n",
    "#### Configuration of `core-site.mxl` Using the `odsc` Command Line Tool\n",
    "\n",
    "With an OCI configuration file, you can run `odsc core-site config -o`. By default, the file uses the OCI configuration file stored in `~/.oci/config`, automatically populates `core-site.xml`, and saves it to `~/spark_conf_dir/core-site.xml`. \n",
    "\n",
    "You can use these command line options \n",
    "- `-a`, `--authentication` Authentication mode. Supports `resource_principal` and `api_key` (default).\n",
    "- `-c`, `--configuration` Path to the OCI configuration file.\n",
    "- `-p`, `--profile` Name of the profile.\n",
    "- `-r`, `--region` Name of the region.\n",
    "- `-o`, `--overwrite` Overwrite `core-site.xml`.\n",
    "- `-O`, `--output` Output path for `core-site.xml`.\n",
    "- `-q`, `--quiet` Suppress non-error output.\n",
    "\n",
    "Run `odsc core-site config --help` to check the use of this CLI using the command line.\n",
    "\n",
    "<a id='manually_update_coresite_api_keys'></a>\n",
    "#### Manually Configuring `core-site.xml`\n",
    "When the conda environment is installed, a templated version of `core-site.xml` is also installed. You can manually update this file.\n",
    "\n",
    "You must specify the following `core-site.xml` file parameters:\n",
    "\n",
    "`fs.oci.client.hostname`: Address of Object Storage. For example, `https://objectstorage.us-ashburn-1.oraclecloud.com`. You must replace us-ashburn-1 with the region you are in.\n",
    "\n",
    "`fs.oci.client.auth.tenantId`: OCID of your tenancy.\n",
    "\n",
    "`fs.oci.client.auth.userId`: Your user OCID.\n",
    "\n",
    "`fs.oci.client.auth.fingerprint`: Fingerprint for the key pair being used.\n",
    "\n",
    "`fs.oci.client.auth.pemfilepath`: The full path and file name of the private key used for authentication. \n",
    "\n",
    "The values of these parameters are found in the OCI configuration file.\n",
    "\n",
    "The following is an example `core-site.xml` file that has been updated. Put all the parameter values between the `<value>` and `</value>` tags:\n",
    "\n",
    "```{xml}\n",
    "<configuration><!-- reference: https://docs.cloud.oracle.com/en-us/iaas/Content/API/SDKDocs/hdfsconnector.htm -->\n",
    "  <property>\n",
    "    <name>fs.oci.client.hostname</name>\n",
    "    <value>https://objectstorage.us-ashburn-1.oraclecloud.com</value>\n",
    "  </property>\n",
    "  <!--<property>-->\n",
    "    <!--<name>fs.oci.client.hostname.myBucket.myNamespace</name>-->\n",
    "    <!--<value></value>&lt;!&ndash; myBucket@myNamespace &ndash;&gt;-->\n",
    "  <!--</property>-->\n",
    "  <property>\n",
    "    <name>fs.oci.client.auth.tenantId</name>\n",
    "    <value>ocid1.tenancy.oc1..aaaaaaaa25c5a2zpfki3wo4ofza5l72aehvwkzzzzz...</value> \n",
    "  </property>\n",
    "  <property>\n",
    "    <name>fs.oci.client.auth.userId</name>\n",
    "    <value>ocid1.user.oc1..aaaaaaaacdxbfmyhe7sxc6iwi73okzuf3src6zzzzzz...</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>fs.oci.client.auth.fingerprint</name>\n",
    "    <value>01:01:02:03:05:08:13:1b:2e:49:77:c0:01:37:01:f7</value>\n",
    "  </property>\n",
    "  <property>\n",
    "    <name>fs.oci.client.auth.pemfilepath</name>\n",
    "    <value>/home/datascience/.oci/key.pem</value>\n",
    "  </property>\n",
    "</configuration>\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='conda_configuration_testing'></a>\n",
    "### Testing the Configuration\n",
    "\n",
    "Set up a spark session in your PySpark conda environment to test if the configuration has been set up properly.  Run the following cells, and ensure that there are no errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Python Spark SQL basic example\") \\\n",
    "    .config(\"spark.driver.cores\", str(1)) \\\n",
    "    .config(\"spark.executor.cores\", str(4)) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, load a CSV file from a public bucket:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# result is a Spark DataFrame\n",
    "#\n",
    "berlin_airbnb = spark\\\n",
    "      .read\\\n",
    "      .format(\"csv\")\\\n",
    "      .option(\"header\", \"true\")\\\n",
    "      .option(\"multiLine\", \"true\")\\\n",
    "      .load(\"oci://oow_2019_dataflow_lab@bigdatadatasciencelarge/usercontent/kaggle_berlin_airbnb_listings_summary.csv\")\\\n",
    "      .cache() # cache the dataset to increase computing speed\n",
    "\n",
    "# the dataframe as a sql view so we can perform SQL on it\n",
    "berlin_airbnb.createOrReplaceTempView(\"berlin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['id',\n",
       " 'listing_url',\n",
       " 'scrape_id',\n",
       " 'last_scraped',\n",
       " 'name',\n",
       " 'summary',\n",
       " 'space',\n",
       " 'description',\n",
       " 'experiences_offered',\n",
       " 'neighborhood_overview',\n",
       " 'notes',\n",
       " 'transit',\n",
       " 'access',\n",
       " 'interaction',\n",
       " 'house_rules',\n",
       " 'thumbnail_url',\n",
       " 'medium_url',\n",
       " 'picture_url',\n",
       " 'xl_picture_url',\n",
       " 'host_id',\n",
       " 'host_url',\n",
       " 'host_name',\n",
       " 'host_since',\n",
       " 'host_location',\n",
       " 'host_about',\n",
       " 'host_response_time',\n",
       " 'host_response_rate',\n",
       " 'host_acceptance_rate',\n",
       " 'host_is_superhost',\n",
       " 'host_thumbnail_url',\n",
       " 'host_picture_url',\n",
       " 'host_neighbourhood',\n",
       " 'host_listings_count',\n",
       " 'host_total_listings_count',\n",
       " 'host_verifications',\n",
       " 'host_has_profile_pic',\n",
       " 'host_identity_verified',\n",
       " 'street',\n",
       " 'neighbourhood',\n",
       " 'neighbourhood_cleansed',\n",
       " 'neighbourhood_group_cleansed',\n",
       " 'city',\n",
       " 'state',\n",
       " 'zipcode',\n",
       " 'market',\n",
       " 'smart_location',\n",
       " 'country_code',\n",
       " 'country',\n",
       " 'latitude',\n",
       " 'longitude',\n",
       " 'is_location_exact',\n",
       " 'property_type',\n",
       " 'room_type',\n",
       " 'accommodates',\n",
       " 'bathrooms',\n",
       " 'bedrooms',\n",
       " 'beds',\n",
       " 'bed_type',\n",
       " 'amenities',\n",
       " 'square_feet',\n",
       " 'price',\n",
       " 'weekly_price',\n",
       " 'monthly_price',\n",
       " 'security_deposit',\n",
       " 'cleaning_fee',\n",
       " 'guests_included',\n",
       " 'extra_people',\n",
       " 'minimum_nights',\n",
       " 'maximum_nights',\n",
       " 'calendar_updated',\n",
       " 'has_availability',\n",
       " 'availability_30',\n",
       " 'availability_60',\n",
       " 'availability_90',\n",
       " 'availability_365',\n",
       " 'calendar_last_scraped',\n",
       " 'number_of_reviews',\n",
       " 'first_review',\n",
       " 'last_review',\n",
       " 'review_scores_rating',\n",
       " 'review_scores_accuracy',\n",
       " 'review_scores_cleanliness',\n",
       " 'review_scores_checkin',\n",
       " 'review_scores_communication',\n",
       " 'review_scores_location',\n",
       " 'review_scores_value',\n",
       " 'requires_license',\n",
       " 'license',\n",
       " 'jurisdiction_names',\n",
       " 'instant_bookable',\n",
       " 'is_business_travel_ready',\n",
       " 'cancellation_policy',\n",
       " 'require_guest_profile_picture',\n",
       " 'require_guest_phone_verification',\n",
       " 'calculated_host_listings_count',\n",
       " 'reviews_per_month']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "berlin_airbnb.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also use `sparksql-magic` to run a query on the view, and store the results as a dataframe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparksql_magic\n",
    "%config SparkSql.max_num_rows=20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache dataframe with lazy load\n",
      "create temporary view `result`\n",
      "capture dataframe to local variable `df`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">name</td><td style=\"font-weight: bold\">latitude</td><td style=\"font-weight: bold\">longitude</td></tr><tr><td>Berlin-Mitte Value! Quiet courtyard/very central</td><td>52.53453732241747</td><td>13.402556926822387</td></tr><tr><td>Prenzlauer Berg close to Mauerpark</td><td>52.54851279221664</td><td>13.404552826587466</td></tr><tr><td>Fabulous Flat in great Location</td><td>52.534996191586714</td><td>13.417578665333295</td></tr><tr><td>BerlinSpot Schoneberg near KaDeWe</td><td>52.498854933130026</td><td>13.34906453348717</td></tr><tr><td>BrightRoom with sunny greenview!</td><td>52.5431572633131</td><td>13.415091104515707</td></tr><tr><td>Geourgeous flat - outstanding views</td><td>52.533030768026826</td><td>13.416046823956403</td></tr><tr><td>Apartment in Prenzlauer Berg</td><td>52.547846407992154</td><td>13.405562243722455</td></tr><tr><td>APARTMENT TO RENT</td><td>52.51051399601544</td><td>13.457850238106195</td></tr><tr><td>In the Heart of Berlin - Kreuzberg</td><td>52.50479227385915</td><td>13.435101853886051</td></tr><tr><td>Downtown Above The Roofs In Berlin</td><td>52.52907092467378</td><td>13.412843393984936</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql --cache --view result df \n",
    "\n",
    "SELECT name, latitude, longitude FROM berlin LIMIT 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache dataframe with lazy load\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">count(1)</td></tr><tr><td>22552</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql --cache\n",
    "\n",
    "SELECT count(*) FROM berlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(name='Berlin-Mitte Value! Quiet courtyard/very central', latitude='52.53453732241747', longitude='13.402556926822387'),\n",
       " Row(name='Prenzlauer Berg close to Mauerpark', latitude='52.54851279221664', longitude='13.404552826587466'),\n",
       " Row(name='Fabulous Flat in great Location', latitude='52.534996191586714', longitude='13.417578665333295'),\n",
       " Row(name='BerlinSpot Schoneberg near KaDeWe', latitude='52.498854933130026', longitude='13.34906453348717'),\n",
       " Row(name='BrightRoom with sunny greenview!', latitude='52.5431572633131', longitude='13.415091104515707'),\n",
       " Row(name='Geourgeous flat - outstanding views', latitude='52.533030768026826', longitude='13.416046823956403'),\n",
       " Row(name='Apartment in Prenzlauer Berg', latitude='52.547846407992154', longitude='13.405562243722455'),\n",
       " Row(name='APARTMENT TO RENT', latitude='52.51051399601544', longitude='13.457850238106195'),\n",
       " Row(name='In the Heart of Berlin - Kreuzberg', latitude='52.50479227385915', longitude='13.435101853886051'),\n",
       " Row(name='Downtown Above The Roofs In Berlin', latitude='52.52907092467378', longitude='13.412843393984936')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try with another file, in one of our buckets\n",
    "\n",
    "**TIP**: \n",
    "* URL FORMAT is oci://{BUCKET}@{NAMESPACE}/{OBJECT_NAME}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "orcl_attrition = spark\\\n",
    "      .read\\\n",
    "      .format(\"csv\")\\\n",
    "      .option(\"header\", \"true\")\\\n",
    "      .load(\"oci://drift_input@frqap2zhtzbe/reference.csv\")\\\n",
    "      .cache() # cache the dataset to increase computing speed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the dataframe as a sql view so we can perform SQL on it\n",
    "orcl_attrition.createOrReplaceTempView(\"ATTRITION\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache dataframe with lazy load\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">count(1)</td></tr><tr><td>1176</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql --cache\n",
    "\n",
    "SELECT count(*) FROM ATTRITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache dataframe with lazy load\n",
      "only showing top 20 row(s)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">TravelForWork</td><td style=\"font-weight: bold\">MonthlyRate</td><td style=\"font-weight: bold\">PercentSalaryHike</td><td style=\"font-weight: bold\">CommuteLength</td><td style=\"font-weight: bold\">SalaryLevel</td><td style=\"font-weight: bold\">YearsOnJob</td><td style=\"font-weight: bold\">JobInvolvement</td><td style=\"font-weight: bold\">PerformanceRating</td><td style=\"font-weight: bold\">Gender</td><td style=\"font-weight: bold\">TrainingTimesLastYear</td><td style=\"font-weight: bold\">YearsSinceLastPromotion</td><td style=\"font-weight: bold\">EnvironmentSatisfaction</td><td style=\"font-weight: bold\">YearsinIndustry</td><td style=\"font-weight: bold\">JobLevel</td><td style=\"font-weight: bold\">JobRole</td><td style=\"font-weight: bold\">WorkLifeBalance</td><td style=\"font-weight: bold\">Age</td><td style=\"font-weight: bold\">RelationshipSatisfaction</td><td style=\"font-weight: bold\">MaritalStatus</td><td style=\"font-weight: bold\">YearsAtCurrentLevel</td><td style=\"font-weight: bold\">HourlyRate</td><td style=\"font-weight: bold\">MonthlyIncome</td><td style=\"font-weight: bold\">OverTime</td><td style=\"font-weight: bold\">JobSatisfaction</td><td style=\"font-weight: bold\">EducationField</td><td style=\"font-weight: bold\">JobFunction</td><td style=\"font-weight: bold\">EducationalLevel</td><td style=\"font-weight: bold\">NumCompaniesWorked</td><td style=\"font-weight: bold\">StockOptionLevel</td><td style=\"font-weight: bold\">YearsWithCurrManager</td></tr><tr><td>infrequent</td><td>19146</td><td>22</td><td>2</td><td>5640</td><td>2</td><td>2</td><td>4</td><td>Male</td><td>2</td><td>2</td><td>4</td><td>4</td><td>2</td><td>Manufacturing Director</td><td>1</td><td>23</td><td>1</td><td>Married</td><td>2</td><td>33</td><td>4775</td><td>No</td><td>4</td><td>Life Sciences</td><td>Software Developer</td><td>L2</td><td>6</td><td>2</td><td>2</td></tr><tr><td>none</td><td>3395</td><td>23</td><td>2</td><td>5678</td><td>23</td><td>2</td><td>4</td><td>Male</td><td>3</td><td>14</td><td>3</td><td>25</td><td>3</td><td>Healthcare Representative</td><td>2</td><td>46</td><td>4</td><td>Married</td><td>15</td><td>74</td><td>10748</td><td>No</td><td>3</td><td>Life Sciences</td><td>Software Developer</td><td>L1</td><td>3</td><td>1</td><td>4</td></tr><tr><td>infrequent</td><td>4510</td><td>18</td><td>15</td><td>2022</td><td>5</td><td>3</td><td>3</td><td>Female</td><td>2</td><td>4</td><td>2</td><td>7</td><td>1</td><td>Research Scientist</td><td>3</td><td>57</td><td>1</td><td>Married</td><td>4</td><td>72</td><td>4963</td><td>Yes</td><td>2</td><td>Life Sciences</td><td>Software Developer</td><td>L4</td><td>9</td><td>3</td><td>3</td></tr><tr><td>none</td><td>17071</td><td>16</td><td>25</td><td>6782</td><td>1</td><td>4</td><td>3</td><td>Female</td><td>2</td><td>0</td><td>2</td><td>22</td><td>4</td><td>Sales Executive</td><td>2</td><td>41</td><td>4</td><td>Single</td><td>0</td><td>100</td><td>13194</td><td>Yes</td><td>2</td><td>Life Sciences</td><td>Product Management</td><td>L3</td><td>4</td><td>0</td><td>0</td></tr><tr><td>infrequent</td><td>18725</td><td>23</td><td>10</td><td>1980</td><td>4</td><td>3</td><td>4</td><td>Male</td><td>4</td><td>0</td><td>4</td><td>10</td><td>1</td><td>Laboratory Technician</td><td>3</td><td>52</td><td>2</td><td>Married</td><td>2</td><td>96</td><td>2075</td><td>No</td><td>4</td><td>Life Sciences</td><td>Software Developer</td><td>L4</td><td>3</td><td>2</td><td>3</td></tr><tr><td>infrequent</td><td>17312</td><td>11</td><td>3</td><td>1376</td><td>22</td><td>3</td><td>3</td><td>Male</td><td>2</td><td>4</td><td>1</td><td>24</td><td>5</td><td>Manager</td><td>2</td><td>43</td><td>1</td><td>Married</td><td>6</td><td>56</td><td>18880</td><td>No</td><td>3</td><td>Life Sciences</td><td>Software Developer</td><td>L3</td><td>5</td><td>0</td><td>14</td></tr><tr><td>infrequent</td><td>12449</td><td>12</td><td>20</td><td>5406</td><td>6</td><td>2</td><td>3</td><td>Male</td><td>2</td><td>3</td><td>4</td><td>6</td><td>1</td><td>Laboratory Technician</td><td>3</td><td>29</td><td>3</td><td>Married</td><td>5</td><td>78</td><td>3196</td><td>No</td><td>1</td><td>Medical</td><td>Software Developer</td><td>L4</td><td>1</td><td>3</td><td>3</td></tr><tr><td>infrequent</td><td>23213</td><td>18</td><td>25</td><td>2160</td><td>1</td><td>1</td><td>3</td><td>Male</td><td>3</td><td>0</td><td>3</td><td>1</td><td>1</td><td>Laboratory Technician</td><td>1</td><td>27</td><td>2</td><td>Single</td><td>0</td><td>66</td><td>2340</td><td>Yes</td><td>4</td><td>Technical Degree</td><td>Software Developer</td><td>L3</td><td>1</td><td>0</td><td>0</td></tr><tr><td>infrequent</td><td>20328</td><td>16</td><td>12</td><td>6618</td><td>0</td><td>2</td><td>3</td><td>Female</td><td>3</td><td>0</td><td>4</td><td>6</td><td>2</td><td>Sales Executive</td><td>3</td><td>57</td><td>3</td><td>Married</td><td>0</td><td>89</td><td>5380</td><td>No</td><td>1</td><td>Marketing</td><td>Product Management</td><td>L5</td><td>4</td><td>1</td><td>0</td></tr><tr><td>infrequent</td><td>16375</td><td>11</td><td>6</td><td>6796</td><td>3</td><td>3</td><td>3</td><td>Male</td><td>2</td><td>1</td><td>4</td><td>21</td><td>5</td><td>Research Director</td><td>3</td><td>48</td><td>2</td><td>Married</td><td>2</td><td>42</td><td>18300</td><td>No</td><td>3</td><td>Life Sciences</td><td>Software Developer</td><td>L5</td><td>4</td><td>1</td><td>1</td></tr><tr><td>infrequent</td><td>4956</td><td>23</td><td>3</td><td>6090</td><td>5</td><td>3</td><td>4</td><td>Male</td><td>3</td><td>0</td><td>3</td><td>6</td><td>1</td><td>Research Scientist</td><td>3</td><td>30</td><td>4</td><td>Married</td><td>4</td><td>76</td><td>2703</td><td>No</td><td>2</td><td>Life Sciences</td><td>Software Developer</td><td>L3</td><td>0</td><td>1</td><td>4</td></tr><tr><td>infrequent</td><td>20338</td><td>12</td><td>20</td><td>3108</td><td>10</td><td>2</td><td>3</td><td>Female</td><td>3</td><td>8</td><td>2</td><td>10</td><td>1</td><td>Research Scientist</td><td>3</td><td>35</td><td>2</td><td>Married</td><td>9</td><td>35</td><td>2929</td><td>No</td><td>4</td><td>Life Sciences</td><td>Software Developer</td><td>L3</td><td>1</td><td>0</td><td>7</td></tr><tr><td>infrequent</td><td>20165</td><td>18</td><td>10</td><td>3686</td><td>1</td><td>3</td><td>3</td><td>Male</td><td>5</td><td>0</td><td>1</td><td>8</td><td>1</td><td>Laboratory Technician</td><td>3</td><td>51</td><td>2</td><td>Married</td><td>0</td><td>64</td><td>2380</td><td>No</td><td>4</td><td>Life Sciences</td><td>Software Developer</td><td>L3</td><td>4</td><td>0</td><td>0</td></tr><tr><td>none</td><td>25043</td><td>12</td><td>24</td><td>2242</td><td>3</td><td>3</td><td>3</td><td>Male</td><td>3</td><td>1</td><td>2</td><td>17</td><td>2</td><td>Laboratory Technician</td><td>4</td><td>45</td><td>3</td><td>Married</td><td>2</td><td>67</td><td>2042</td><td>No</td><td>2</td><td>Medical</td><td>Software Developer</td><td>L3</td><td>4</td><td>1</td><td>2</td></tr><tr><td>often</td><td>5530</td><td>13</td><td>25</td><td>2452</td><td>2</td><td>3</td><td>3</td><td>Male</td><td>3</td><td>2</td><td>4</td><td>3</td><td>1</td><td>Sales Representative</td><td>3</td><td>25</td><td>3</td><td>Married</td><td>2</td><td>38</td><td>2400</td><td>No</td><td>4</td><td>Medical</td><td>Product Management</td><td>L3</td><td>0</td><td>2</td><td>1</td></tr><tr><td>often</td><td>5972</td><td>17</td><td>11</td><td>4026</td><td>5</td><td>3</td><td>3</td><td>Male</td><td>4</td><td>2</td><td>4</td><td>5</td><td>1</td><td>Research Scientist</td><td>4</td><td>35</td><td>4</td><td>Divorced</td><td>3</td><td>43</td><td>3815</td><td>Yes</td><td>3</td><td>Medical</td><td>Software Developer</td><td>L4</td><td>1</td><td>1</td><td>0</td></tr><tr><td>infrequent</td><td>26914</td><td>12</td><td>15</td><td>5076</td><td>18</td><td>3</td><td>3</td><td>Female</td><td>2</td><td>12</td><td>4</td><td>18</td><td>1</td><td>Laboratory Technician</td><td>2</td><td>38</td><td>3</td><td>Divorced</td><td>7</td><td>95</td><td>3034</td><td>No</td><td>1</td><td>Life Sciences</td><td>Software Developer</td><td>L3</td><td>1</td><td>1</td><td>17</td></tr><tr><td>infrequent</td><td>12477</td><td>11</td><td>19</td><td>6292</td><td>3</td><td>4</td><td>3</td><td>Male</td><td>3</td><td>1</td><td>1</td><td>6</td><td>2</td><td>Sales Executive</td><td>2</td><td>26</td><td>2</td><td>Married</td><td>2</td><td>93</td><td>6232</td><td>No</td><td>3</td><td>Life Sciences</td><td>Product Management</td><td>L1</td><td>2</td><td>0</td><td>2</td></tr><tr><td>infrequent</td><td>10732</td><td>14</td><td>21</td><td>5718</td><td>3</td><td>3</td><td>3</td><td>Male</td><td>2</td><td>0</td><td>4</td><td>10</td><td>2</td><td>Sales Executive</td><td>3</td><td>30</td><td>4</td><td>Divorced</td><td>2</td><td>45</td><td>6931</td><td>No</td><td>4</td><td>Marketing</td><td>Product Management</td><td>L2</td><td>2</td><td>1</td><td>2</td></tr><tr><td>infrequent</td><td>18830</td><td>18</td><td>2</td><td>1220</td><td>2</td><td>3</td><td>3</td><td>Female</td><td>3</td><td>2</td><td>4</td><td>2</td><td>1</td><td>Research Scientist</td><td>2</td><td>26</td><td>4</td><td>Single</td><td>2</td><td>40</td><td>2096</td><td>No</td><td>2</td><td>Medical</td><td>Software Developer</td><td>L3</td><td>1</td><td>0</td><td>1</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql --cache\n",
    "\n",
    "SELECT * FROM ATTRITION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cache dataframe with lazy load\n",
      "create temporary view `result`\n",
      "capture dataframe to local variable `df_gender`\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table><tr style=\"border-bottom: 1px solid\"><td style=\"font-weight: bold\">gender</td><td style=\"font-weight: bold\">count(1)</td></tr><tr><td>Female</td><td>479</td></tr><tr><td>Male</td><td>697</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%sparksql --cache --view result df_gender \n",
    "\n",
    "SELECT gender, count(*) FROM ATTRITION GROUP BY gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------+\n",
      "|gender|count(1)|\n",
      "+------+--------+\n",
      "|Female|     479|\n",
      "|  Male|     697|\n",
      "+------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_gender.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:pyspark32_p38_cpu_v1]",
   "language": "python",
   "name": "conda-env-pyspark32_p38_cpu_v1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
